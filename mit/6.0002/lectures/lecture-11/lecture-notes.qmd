---
title: MIT 6.0002 - Lecture 11 Notes
format: html
jupyter: python3
---

[Source](https://ocw.mit.edu/courses/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/resources/mit6_0002f16_lec11/)

Topics covered in this lecture:

- Machine Learning Introduction
- Classifying and Clustering
- Machine Learning Methods

# Machine Learning (Introduction)

- Early Definition (1959):
  - Field of study that gives computers the ability to learn without being explicitly programmed
- Data and Output are fed to a computer and a program is produced
  - That program and additional data are then used to create more output
  - From there, the output and data are used to refine the program, and so on...
- How are things learned?
  - Declarative Knowledge (Memorization)
  - Imperative Knowledge (Generalization)
  - Want to make programs that can infer useful information from **implicit** pattern in data

## Basic Paradigm

- Observe a set of examples (*training data*)
- Infer something about the process that generated that data (*model*)
- Use inference to make predictions about previously unseen data (*test data*)
- Some variations
  - **Supervised**: data is labeled and want to predict those labels
  - **Unsupervised**: data is not labeled, want to find natural "clusters" in the data

# Classifying & Clustering

## Data

```{python}
from matplotlib import pyplot as plt

#> Data

##> Receivers
edelman = ['edelman', 70, 200]
hogan = ['hogan', 73, 210]
gronkowski = ['gronkowski', 78, 265]
amendola = ['amendola', 71, 190]
bennett = ['bennett', 78, 275]

##> Linemen
cannon = ['cannon', 77, 335]
solder = ['solder', 80, 325]
mason = ['mason', 73, 310]
thuney = ['thuney', 77, 305]
karras = ['karras', 76, 305]

##> Process
receivers = [edelman, hogan, gronkowski, amendola, bennett]
linemen = [cannon, solder, mason, thuney, karras]

receiver_names = [player[0] for player in receivers]
linemen_names = [player[0] for player in linemen] 

players = [edelman, hogan, gronkowski, amendola, bennett, cannon, solder, mason, thuney, karras]

names = [player[0] for player in players]
heights = [player[1] for player in players]
weights = [player[2] for player in players]

plt.scatter(heights, weights)
plt.show()
```

## Goal

- Want to decide on "similarity" of examples, with goal of separating into distinct "natural" groups
- "Similarity" is a distance metric

- Similarity based on weight:

```{python}
plt.scatter(heights, weights)
plt.axhline(y=240, color='red', linestyle='--')
plt.show()
```

- Similarity based on hight:

```{python}
plt.scatter(heights, weights)
plt.axvline(x=75, color='red', linestyle='--')
plt.show()
```

- Both Attributes

```{python}
import numpy as np

plt.scatter(heights, weights)

x = np.linspace(60, 90, 100)
y = 615 + (-5 * x)

plt.plot(x, y, color='red', linestyle='--')
plt.show()
```

- What if the data was labeled?

```{python}
from matplotlib import pyplot as plt

red = (1, 0, 0)
blue = (0, 0, 1)

color = [red if name in receiver_names else blue for name in names]

plt.scatter(heights, weights, c=color)
plt.show()
```

- Given labeled groups in a feature space, we want to find a subsurface in that space that separates the groups
  - When examples are well separated, this is straightforward
  - When examples in labeled groups overlap, we may have to trade off false positives and false negatives
- In this case, a straight line appears to be the most obvious separator
```{python}
plt.scatter(heights, weights, c=color)
plt.axhline(y=290, color='red', linestyle='--')
plt.show()
```

- What if we added new data?

```{python}
#> Running Backs
blount = ['blount', 72, 250]
white = ['white', 70, 205]

backs = [blount, white]

for back in backs:
  names.append(back[0])
  heights.append(back[1])
  weights.append(back[2])

black = (0, 0, 0)

color = []
for name in names:
  if name in receiver_names:
    color.append(red)
  elif name in linemen_names:
    color.append(blue)
  else:
    color.append(black)

plt.scatter(heights, weights, c=color)
plt.show()
```

- One of the new observations is right on the line, which may be ambiguous

```{python}
plt.scatter(heights, weights, c=color)
plt.plot(x, y, color='red', linestyle='--')
plt.show()
```

- Using the horizontal line still shows a good fit (but perhaps overfitting?)

```{python}
plt.scatter(heights, weights, c=color)
plt.axhline(y=290, color='red', linestyle='--')
plt.show()
```

# Machine Learning Methods

- Models based on unlabeled data
  - Clustering training data into groups of nearby points
  - Resulting clusters can assign labels to new data

- Models based on labeled data
  - Separate labeled groups of similar data from other groups
  - May not be possible to perfectly separate groups without "over fitting"
  - Can make decisions with respect to trading off "false positives" versus "false negatives"
  - Resulting classifiers can assign labels to new data

- All methods require:
  - Choosing training data and evaluation method
  - Representation of the features
  - Distance metric for feature vectors
  - Objective function and constraints
  - Optimization method for teaching the model

## Feature Representation

- Features never fully describe the situation
- Feature Engineering
  - Represent examples by feature vectors that will facilitate generalization
  - Some features may be helpful while others may cause overfitting
- Want to maximize ratio of useful input to irrelevant input (Signal-to-Noise Ratio)

### Example
- The features below are informative but do not allow for a simple rule to correctly classify all animals as a reptile or not
- Closest we can get is:
  1. Has scales
  2. Is Cold-Blooded

| Name            | Egg-Laying | Scales | Poisonous | Cold-Blooded | # Legs | Reptile |
|:---------------:|:----------:|:------:|:---------:|:------------:|:------:|:-------:|
| Cobra           | True       | True   | True      | True         | True   | Yes     |
| Rattlesnake     | True       | True   | True      | True         | True   | Yes     |
| Boa Constrictor | False      | True   | False     | True         | True   | Yes     |
| Chicken         | True       | True   | False     | False        | 2      | No      |
| Alligator       | True       | True   | False     | True         | 4      | Yes     |
| Dart Frog       | True       | False  | True      | False        | 4      | No      |
| Salmon          | True       | True   | False     | True         | True   | No      |
| Python          | True       | True   | False     | True         | True   | Yes     |

### Measuring Distance

- One way to separate data points is to measure the distance between their features
  - Can cluster "nearby" examples into a common class (unlabeled data)
  - Can find a classifier surgace of examples that optimally separates (labeled data)

- Minkowski Metric
  - When p = 1: Manhattan Distance
  - When p = 2: Euclidean Distance

$$
dist(X_1, X_2, p) = (\sum_{k=1}^{length} abs(X_{1_k} - X_{2_k}) ^ p) ^ {1/p}
$$

```{python}
import pylab

def minkowskiDist(v1, v2, p):
    """Assumes v1 and v2 are equal-length arrays of numbers
       Returns Minkowski distance of order p between v1 and v2"""
    dist = 0.0
    for i in range(len(v1)):
        dist += abs(v1[i] - v2[i]) ** p
    
    return dist ** (1.0 / p)

class Animal(object):
    def __init__(self, name, features):
        """Assumes name a string; features a list of numbers"""
        self.name = name
        self.features = np.array(features)
        
    def getName(self):
        return self.name
    
    def getFeatures(self):
        return self.features
    
    def distance(self, other):
        """Assumes other an Animal
           Returns the Euclidean distance between feature vectors
              of self and other"""
        return minkowskiDist(self.getFeatures(), other.getFeatures(), 2)

def compareAnimals(animals, precision):
    """Assumes animals is a list of animals, precision an int >= 0
       Builds a table of Euclidean distance between each animal"""
    #Get labels for columns and rows
    columnLabels = []
    for a in animals:
        columnLabels.append(a.getName())
    
    rowLabels = columnLabels[:]
    tableVals = []
    
    #Get distances between pairs of animals
    #For each row
    for a1 in animals:
        row = []
        
        #For each column
        for a2 in animals:
            if a1 == a2:
                row.append('--')
            else:
                distance = a1.distance(a2)
                row.append(str(round(distance, precision)))
        tableVals.append(row)
    
    #Produce table
    table = pylab.table(
      rowLabels = rowLabels,
      colLabels = columnLabels,
      cellText = tableVals,
      cellLoc = 'center',
      loc = 'center',
      colWidths = [0.2] * len(animals)
    )
    table.scale(1, 2.5)
    pylab.title('Eucliedan Distance Between Animals')
    pylab.axis('off')
```

**Euclidean Distance**

- Rattlesnake and Boa Constrictor are much closer to each other than they are to the dart frog

```{python}
rattlesnake = Animal('rattlesnake', [1,1,1,1,0])
boa = Animal('boa\nconstrictor', [0,1,0,1,0])
dartFrog = Animal('dart frog', [1,0,1,0,4])

animals = [rattlesnake, boa, dartFrog]

compareAnimals(animals, 3)
```

- Adding an alligator results in it being closer to the dart frog than the snakes
  - Alligator differs from the frog in 3 features, but only in 2 featurs with the Boa
  - The problem is that the scale on "legs" is from 0 to 4, rather than 0 to 1 for the others
  - The "legs" feature is disproportionately large

```{python}
alligator = Animal('alligator', [1,1,0,1,4])
animals.append(alligator)

compareAnimals(animals, 3)
```

- If binary features were used instead, the alligator would be much closer to the snakes than to the dart frogs
  - The final feature is "has legs" rather than "number of legs"

```{python}
rattlesnake = Animal('rattlesnake', [1,1,1,1,0])
boa = Animal('boa\nconstrictor', [0,1,0,1,0])
dartFrog = Animal('dart frog', [1,0,1,0,1])
alligator = Animal('alligator', [1,1,0,1,1])

animals = [rattlesnake, boa, dartFrog, alligator]

compareAnimals(animals, 3)
```

## Supervised vs Unsupervised

- When given unlabeled data, try to find clusters of examples near each other
  - Use centroids of the clusters as definition to each learned class
  - Assign new data points to the closest cluster
- When given labeled data, learn mathematical surface that "best" separates labeled examples
  - Subject to the contraints on complexity of surface (*i.e.*, do not overfit)
  - New data assigned to class based on portion of feature space carved out by classifier surface

- Learned models will depend on:
  - Distance metric between examples
  - Choice of feature vectors
  - Constraints on complexity of model

## Classification Metrics

- Confusion Matrices
  - Shows the actual labels vs the predicted labels
  - Four possible outcomes:
    1. Predicted True / Actually True: True Positive
    2. Predicted True / Actually False: False Positive
    3. Predicted False / Actually True: False Negative
    4. Predicted False / Actually False: True Negative
- Model Accuracy

$$
accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

- Positive Predictive Value

$$
PPV = \frac{TP}{TP + FP}
$$

- Sensitivity (percent correctly found)

$$
sensitivity = \frac{TP}{TP + FN}
$$

- Specificity (percentage correctly rejected)

$$
specificity = \frac{TN}{TN + FP}
$$

# Summary

- Machine learning methods provide a way of building models of processes from data sets
- Supervised learning uses labeled data and creates classifiers that optimally separate data into known classes
- Unsupervised learning tries to infer latent variables by clustering training examples into nearby groups
- Choice of feature influences the results
- Choice of distance measurements between examples influences results
