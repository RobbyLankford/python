---
title: MIT 6.0002 - Lecture 13 Notes
format: html
jupyter: python3
---

[Source](https://ocw.mit.edu/courses/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/resources/mit6_0002f16_lec13/)

Topics covered in this lecture:

- Supervised Learning
- Classification
  - K-Nearest Neighbors
  - Logistic Regression

# Supervised Learning

## Main Approaches

1. Regression
  - Predict a real number associated with a feature vector
  - *E.g.*, use linear regression to fit a curve to data

2. Classification
   1. Predict a discrete value (a label) associated with a feature vector

# Classification

## Simple Example

- Recall the distance matrix for various animals and their features
- Can classify using an algorithm such as nearest neighbor
- When predicting the label of a new example
  - Find the nearest example in the training data
  - Predict the label associated with that example

## K-Nearest Neighbors

- Advantages
  - Fast learning, no explicit training
  - No theory required
  - Easy to explain the method and the results

- Disadvantages
  - Memory intensive and predictions can take a long time
    - However, there do exist better algorithms than simple brute force
  - No model to shed light on the process that generated the data

## Performance Metrics

- Accuracy is often not enough
  - Ex: the Titanic... 62% of passengers and 76% of crew members died
    - If we just label everyone as "died", then we are 62-72% accurate, but not useful
  - Ex: a disease that occurs in only 0.1% of the population
    - If we just label everyone as not having it, we are 99.9% accurate, but not useful

```{python}
def accuracy(truePos, falsePos, trueNeg, falseNeg):
  numerator = truePos + trueNeg
  denominator = truePos + trueNeg + falsePos + falseNeg

  return numerator / denominator
```

- Other metrics are often examined
  - Sensitivity (Recall)
  - Specificity (Precision)
  - Positive Predictive Value
  - Negative Predictive Value

- Sensitivity (Recall)

$$
sensitivity = \frac{TP}{TP + FN}
$$

- Specificity (Precision)

$$
specificity = \frac{TN}{TN + FP}
$$

- Positive Predictive Value

$$
PPV = \frac{TP}{TN + FN}
$$

- Negative Predictive Value

$$
NPV = \frac{TN}{TN + FN}
$$

```{python}
def sensitivity(truePos, falseNeg):
  try:
    return truePos / (truePos + falseNeg)
  except ZeroDivisionError:
    return float('nan')

def specificity(trueNeg, falsePos):
  try:
    return trueNeg / (trueNeg + falsePos)
  except ZeroDivisionError:
    return float('nan')

def posPredVal(truePos, falsePos):
  try:
    return truePos / (truePos + falsePos)
  except ZeroDivisionError:
    return float('nan')

def negPredVal(trueNeg, falseNeg):
  try:
    return trueNeg / (trueNeg + falseNeg)
  except ZeroDivisionError:
    return float('nan')
```

## Testing Methodology

### Leave-One-Out

```{python}
def getStats(truePos, falsePos, trueNeg, falseNeg, toPrint=True):
  accur = accuracy(truePos, falsePos, trueNeg, falseNeg)
  sens = sensitivity(truePos, falseNeg)
  spec = specificity(trueNeg, falsePos)
  ppv = posPredVal(truePos, falsePos)

  if toPrint:
    print(f" Accuracy = {round(accur, 3)}")
    print(f" Sensitivity = {round(sens, 3)}")
    print(f" Specificity = {round(spec, 3)}")
    print(f" Pos. Pred. Val. = {round(ppv, 3)}")
  
  return (accur, sens, spec, ppv)

def leaveOneOut(examples, method, toPrint=True):
  truePos, falsePos, trueNeg, falseNeg = 0, 0, 0, 0

  for i in range(len(examples)):
    testCase = examples[i]
    trainingData = examples[0:i] + examples[i+1: ]
    results = method(trainingData, [testCase])

    truePos += results[0]
    falsePos += results[1]
    trueNeg += results[2]
    falseNeg += results[3]
  
  if toPrint:
    getStats(truePos, falsePos, trueNeg, falseNeg)
  
  return truePos, falsePos, trueNeg, falseNeg
```

### Repeated Random Subsampling

```{python}
import random

def split80_20(examples):
  sampleIndicies = random.sample(range(len(examples)), len(examples) // 5)
  trainingSet, testSet = [], []

  for i in range(len(examples)):
    if i in sampleIndicies:
      testSet.append(examples[i])
    else:
      trainingSet.append(examples[i])
  
  return trainingSet, testSet

def randomSplits(examples, method, numSplits, toPrint=True):
  truePos, falsePos, trueNeg, falseNeg = 0, 0, 0, 0
  random.seed(0)

  for _ in range(numSplits):
    trainingSet, testSet = split80_20(examples)
    results = method(trainingSet, testSet)

    truePos += results[0]
    falsePos += results[1]
    trueNeg += results[2]
    falseNeg += results[3]
  
  if toPrint:
    getStats(truePos, falsePos, trueNeg, falseNeg)
  
  return truePos / numSplits, falsePos / numSplits, trueNeg / numSplits, falseNeg / numSplits
```

## Example: KNN

```{python}
def minkowskiDist(v1, v2, p):
    """Assumes v1 and v2 are equal-length arrays of numbers
       Returns Minkowski distance of order p between v1 and v2"""
    dist = 0.0
    
    for i in range(len(v1)):
        dist += abs(v1[i] - v2[i]) ** p
    
    return dist ** (1 / p)
```

- Will use data on Titanic passengers

```{python}
class Passenger(object):
  featureNames = ('C1', 'C2', 'C3', 'age', 'male gender')

  def __init__(self, pClass, age, gender, survived, name):
    self.name = name
    self.featureVec = [0, 0, 0, age, gender]
    self.featureVec[pClass - 1] = 1
    self.label = survived
    self.cabinClass = pClass
  
  def distance(self, other):
    return minkowskiDist(self.featureVec, other.featureVec, 2)
  
  def getClass(self):
    return self.cabinClass
  
  def getAge(self):
    return self.featureVec[3]
  
  def getGender(self):
    return self.featureVec[4]
  
  def getName(self):
    return self.name
  
  def getFeatures(self):
    return self.featureVec[:]
  
  def getLabel(self):
    return self.label

def getTitanicData(fname):
  data = {}
  data['class'], data['survived'], data['age'] = [], [], []
  data['gender'], data['name'] = [], []

  f = open(fname)
  line = f.readline()

  while line != '':
    split = line.split(',')
    data['class'].append(int(split[0]))
    data['age'].append(float(split[1]))

    if split[2] == 'M':
      data['gender'].append(1)
    else:
      data['gender'].append(0)
    
    if split[3] == '1':
      data['survived'].append('Survived')
    else:
      data['survived'].append('Died')
    
    data['name'].append(split[4: ])
    line = f.readline()
  
  return data

def buildTitanicExamples(fname):
  data = getTitanicData(fname)
  examples = []

  for i in range(len(data['class'])):
    p = Passenger(
      data['class'][i],
      data['age'][i],
      data['gender'][i],
      data['survived'][i],
      data['name'][i]
    )

    examples.append(p)
  
  print(f"Finished processing {len(examples)} passengers\n")

  return examples
```

- Functions to run KNN

```{python}
def findKNearest(example, exampleSet, k):
  kNearest, distances = [], []

  for i in range(k):
    kNearest.append(exampleSet[i])
    distances.append(example.distance(exampleSet[i]))
  
  maxDist = max(distances)

  for e in exampleSet[k: ]:
    dist = example.distance(e)
    
    if dist < maxDist:
      maxIndex = distances.index(maxDist)
      kNearest[maxIndex] = e
      distances[maxIndex] = dist
      maxDist = max(distances)
  
  return kNearest, distances

def KNearestClassify(training, testSet, label, k):
  """Assumes training & testSet lists of examples, k an int
     Predicts whether each example in testSet has label
     Returns number of true positives, false positives,
       true negatives, and false negatives"""

  truePos, falsePos, trueNeg, falseNeg = 0, 0, 0, 0

  for testCase in testSet:
    nearest, distances = findKNearest(testCase, training, k)
    numMatch = 0

    for i in range(len(nearest)):
      if nearest[i].getLabel() == label:
        numMatch += 1
      
      if numMatch > k // 2:
        if testCase.getLabel() == label:
          truePos += 1
        else:
          falsePos += 1
      else:
        if testCase.getLabel() != label:
          trueNeg += 1
        else:
          falseNeg += 1
  
  return truePos, falsePos, trueNeg, falseNeg
```

- Run model using 80/20 split and Leave One Out
  - Not much difference between the two

```{python}
examples = buildTitanicExamples('TitanicPassengers.txt')

knn = lambda training, testSet: KNearestClassify(training, testSet, 'Survived', 3)

numSplits = 10

print(f"Average of {numSplits} 80/20 splits using KNN (k = 3)")
truePos, falsePos, trueNeg, falseNeg = randomSplits(examples, knn, numSplits)

print()

print(f"Average of LOO testing using KNN (k = 3)")
truePos, falsePos, trueNeg, falseNeg = leaveOneOut(examples, knn)
```

## Logistic Regression

- Analogous to linear regression
- Designed explicity for predicting the probability of an event
- Finds weights for each feature
  - Positive implies variable positively correlated with outcome
  - Negative implies variable negatively correlated with outcome
  - Absolute magnitude related to strength of the correlation

## Example - Logistic Regression

- Define functions

```{python}
from sklearn import linear_model as lm

def buildModel(examples, toPrint=True):
  featureVecs, labels = [], []

  for e in examples:
    featureVecs.append(e.getFeatures())
    labels.append(e.getLabel())
  
  model = lm.LogisticRegression().fit(featureVecs, labels)

  if toPrint:
    print(f"model.classes_ = {model.classes_}")

    for i in range(len(model.coef_)):
      print(f"For label {model.classes_[1]}")
      
      for j in range(len(model.coef_[0])):
        print(f"    {Passenger.featureNames[j]} = {model.coef_[0][j]}")

  return model

def applyModel(model, testSet, label, prob=0.5):
  testFeatureVecs = [e.getFeatures() for e in testSet]
  probs = model.predict_proba(testFeatureVecs)
  truePos, falsePos, trueNeg, falseNeg = 0, 0, 0, 0

  for i in range(len(probs)):
    if probs[i][1] > prob:
      if testSet[i].getLabel() == label:
        truePos += 1
      else:
        falsePos += 1
    else:
      if testSet[i].getLabel() != label:
        trueNeg += 1
      else:
        falseNeg += 1
  
  return truePos, falsePos, trueNeg, falseNeg
```

- Run model
  - Again, not much difference between methods

```{python}
def lr(trainingData, testData, prob=0.5):
  model = buildModel(trainingData, False)
  results = applyModel(model, testData, 'Survived', prob)

  return results

numSplits = 10
print(f"Average of {numSplits} 80/20 splits LR")
truePos, falsePos, trueNeg, falseNeg = randomSplits(examples, lr, numSplits)

print()

print(f"Average of LOO testing using LR")
truePos, falsePos, trueNeg, falseNeg = leaveOneOut(examples, lr)
```

- Run model and look at feature weights

```{python}
buildModel(examples, True)
```

- Can also change the probability cutoff
  - Default is usually 0.5 to be classified into the "positive" class, but it does not have to be

```{python}
random.seed(0)
trainingSet, testSet = split80_20(examples)
model = buildModel(trainingSet, False)

print("Try p = 0.1")
truePos, falsePos, trueNeg, falseNeg = applyModel(model, testSet, "Survived", 0.1)
getStats(truePos, falsePos, trueNeg, falseNeg)

print()

print("Try p = 0.9")
truePos, falsePos, trueNeg, falseNeg = applyModel(model, testSet, "Survived", 0.9)
getStats(truePos, falsePos, trueNeg, falseNeg)
```

- Lastly, can use the Receiver Operating Characteristic (ROC) Curve and the area under the ROC curve to measure performance

```{python}
from sklearn import metrics as metrics
from matplotlib import pyplot as plt

def buildROC(trainingSet, testSet, title, plot=True):
  model = buildModel(trainingSet, True)
  xVals, yVals = [], []
  
  p = 0.0
  while p <= 1.0:
    truePos, falsePos, trueNeg, falseNeg = applyModel(model, testSet, "Survived", p)
    xVals.append(1.0 - specificity(trueNeg, falsePos))
    yVals.append(sensitivity(truePos, falseNeg))
    p += 0.01
  
  auroc = metrics.auc(xVals, yVals)

  if plot:
    title = f"{title} \nAUROC = {str(round(auroc, 3))}"

    plt.plot(xVals, yVals)
    plt.plot([0, 1], [0, 1])
    plt.title(title)
    plt.xlabel("1 - specificity")
    plt.ylabel("Sensitivity")
  
  return auroc

random.seed(0)
trainingSet, testSet = split80_20(examples)
buildROC(trainingSet, testSet, "ROC for Predicting Survival, 1 Split")
```